ğŸ‡§ğŸ‡­ Bahrain Statistical AI Agent
Automated Data Ingestion Â· Master Dataset Builder Â· AI Query Engine Â· Extensible Analytics Platform
ğŸ“Œ Overview

The Bahrain Statistical AI Agent is an end-to-end system that automatically:

Fetches datasets from government open data portals

Cleans & normalizes them

Merges them safely into unified master datasets

Answers questions using both rule-based logic and LLM (ChatGPT) fallback

Allows clients to drag-and-drop CSV files into data/incoming/

Supports fully automated 6-month scheduled updates

Easily extends into segmentation, forecasting, or mobility analytics

This repository is designed so that even non-technical users can maintain updated national statistics without breaking anything.

ğŸ“‚ Project Structure
   bahrain_stats_agent/
â”‚
â”œâ”€â”€ bahrain_agent/
â”‚   â”œâ”€â”€ agent.py               # Core reasoning engine
â”‚   â”œâ”€â”€ nlu_router.py          # Intent detection + LLM fallback
â”‚   â”œâ”€â”€ describe_layer.py      # Statistical descriptions & summaries
â”‚   â”œâ”€â”€ repo.py                # Repository for master datasets
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fetch_and_ingest.py    # Automatic dataset downloader + ingestion trigger
â”‚   â”œâ”€â”€ ingest_and_prepare.py  # Cleansing, normalization, merging pipeline
â”‚   â”œâ”€â”€ webhook_receiver.py    # Optional real-time ingestion API
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ endpoints.json         # URL list for fetching datasets
â”‚   â”œâ”€â”€ schemas.json           # Schema mapping rules for ingestion
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ incoming/              # Drop new CSVs here (manual or automatic)
â”‚   â”œâ”€â”€ incoming_failed/       # Suspicious files stored safely
â”‚   â”œâ”€â”€ master/                # Final cleaned, unified datasets
â”‚
â”œâ”€â”€ logs/                      # Automatic logs from fetching & ingestion
â”‚
â”œâ”€â”€ architecture.md            # System design & architecture
â””â”€â”€ README.md                  # This file

ğŸ§  Key Features
âœ” Automated Data Ingestion

Downloads datasets from URLs in endpoints.json

Supports retries, file-size limits, deduplication

Performs CSV validation and rejects suspicious files

Automatically triggers ingestion pipeline

âœ” Intelligent Data Normalization

The ingestion pipeline handles:

Column name unpredictability

Nationality / governorate normalization

Year extraction & parsing

Duplicates and row-level validation

Safe merging into master datasets

âœ” Drag-and-Drop Data Support

Client can simply drop CSVs into:
data/incoming/
Then run:
python scripts/ingest_and_prepare.py --run

No formatting needed â€” the system will adapt.

âœ” Hybrid AI Agent

Combines:

Rule-based logic

Structured dataset querying

Large Language Model fallback

Query refinement

Year detection

Intent classification

âœ” Future-Ready Architecture

Built to support:

Housemaid demand segmentation

Labour diagnosis modeling

Mobility pattern integration

Workforce surge/shortage predictions

Nationality cluster analysis

âš™ï¸ Installation
1. Clone the repository
   git clone https://github.com/yourname/bahrain_stats_agent.git
cd bahrain_stats_agent
2. Create a virtual environment
   python -m venv venv
3. Activate the environment
   venv\Scripts\activate
4. Install dependencies
   pip install -r requirements.txt

ğŸš€ Usage Guide
1. Automatic Fetch + Ingestion

Fetch datasets from all URLs in config/endpoints.json:

python scripts/fetch_and_ingest.py --run


Optional dry run (no data is written to master files):

python scripts/fetch_and_ingest.py --run --dry

2. Manual Drag-and-Drop Workflow

Place CSVs into:

data/incoming/


Run ingestion:

python scripts/ingest_and_prepare.py --run


This safely updates master datasets.

3. Automated Every 6 Months (Windows Task Scheduler)
Create a scheduled task:

Program: python

Arguments:

scripts/fetch_and_ingest.py --run


Start in:

C:\path\to\bahrain_stats_agent\


Trigger:
Every 6 months

Your client never needs to do anything manually again.

ğŸ“¡ Optional: Webhook Ingestion

You can run an ingestion API server:

uvicorn scripts.webhook_receiver:app --host 0.0.0.0 --port 8000


Supports:

Upload CSV (multipart)

Provide URL for auto-download

Send raw CSV text

Trigger ingestion via HTTP

ğŸ“ Config Files
endpoints.json

Stores URLs for fetching:

{
  "endpoints": [
    "https://data.gov.bh/â€¦/population.csv",
    "https://data.gov.bh/â€¦/labour.csv"
  ]
}

schemas.json

Stores flexible mappings:

{
  "synonyms": {
    "nationality": ["nat", "nation", "n"],
    "governorate": ["gov", "region", "muharraq"]
  }
}


The ingestion pipeline uses this file to detect how to map incoming CSVs.

ğŸ“Š Master Dataset Philosophy

Each master CSV is designed to be:

Non-destructive

Append-safe

Schema-validated

Human-readable

Machine-consumable

This ensures long-term consistency even with unstable data sources.

ğŸ§© Extending the Model

You can easily plug new modules into:

describe_layer.py
agent.py
repo.py

Examples:

â¤ Housemaid Demand Segmentation

Add describe_domestic_workers() + domain logic.

â¤ Labour Market Forecasting

Add forecast_labour() using statsmodels or ML.

â¤ Mobility Integration

Add mobility_segmentation() using telecom movement files.

ğŸ“ˆ Example Query Flow

User asks:
â€œGive me population density in Muharraq for 2020.â€

nlu_router.py:

Detects entity â†’ population density

Extracts year â†’ 2020

Passes to agent.py

agent.py:

Loads density master dataset

Filters for Muharraq, 2020

Formats structured answer

If missing values â†’ falls back to LLM with safe context.

ğŸ§ª Development Notes

To work safely:

Dry Run:
  
  python scripts/ingest_and_prepare.py --run --dry

Verbose mode:

Enable detailed logging in /logs/.

Duplicate Safety:

The system:

Detects duplicates using MD5 hashing

Keeps the older file

Skips unwanted duplicates

Ensures master dataset stability





